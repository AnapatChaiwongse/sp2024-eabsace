{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from dataclasses import asdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from test_MAMS import predict_val, predict_test\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "font = {'size' : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "# import test_Rest14\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    BartConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    BartModel, #add for https://huggingface.co/facebook/bart-base\n",
    "    pipeline, #add for https://huggingface.co/facebook/bart-large-cnn\n",
    "    GPT2Tokenizer, #add for https://huggingface.co/gpt2\n",
    "    GPT2Model, #add for https://huggingface.co/gpt2\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertModel,\n",
    "    BertTokenizer,\n",
    "    CamembertConfig,\n",
    "    CamembertModel,\n",
    "    CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    "    ElectraConfig,\n",
    "    ElectraModel,\n",
    "    ElectraTokenizer,\n",
    "    EncoderDecoderConfig,\n",
    "    EncoderDecoderModel,\n",
    "    LongformerConfig,\n",
    "    LongformerModel,\n",
    "    LongformerTokenizer,\n",
    "    MarianConfig,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    MobileBertConfig,\n",
    "    MobileBertModel,\n",
    "    MobileBertTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaModel,\n",
    "    RobertaTokenizer,\n",
    "    MBartForConditionalGeneration, #add for https://huggingface.co/facebook/mbart-large-50\n",
    "    MBart50TokenizerFast, #add for https://huggingface.co/facebook/mbart-large-50\n",
    "    get_linear_schedule_with_warmup,\n",
    "    MBartConfig,\n",
    "    GPT2LMHeadModel, #https://github.com/ai-forever/mgpt\n",
    "    GPT2Tokenizer, #https://github.com/ai-forever/mgpt\n",
    "    GPT2Config,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from simpletransformers.config.global_args import global_args\n",
    "from simpletransformers.config.model_args import Seq2SeqArgs\n",
    "from simpletransformers.seq2seq.seq2seq_utils import Seq2SeqDataset, SimpleSummarizationDataset\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    wandb_available = True\n",
    "except ImportError:\n",
    "    wandb_available = False\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"auto\": (AutoConfig, AutoModel, AutoTokenizer),\n",
    "    \"bart\": (BartConfig, BartForConditionalGeneration, BartTokenizer),\n",
    "    \"bert\": (BertConfig, BertModel, BertTokenizer),\n",
    "    \"camembert\": (CamembertConfig, CamembertModel, CamembertTokenizer),\n",
    "    \"distilbert\": (DistilBertConfig, DistilBertModel, DistilBertTokenizer),\n",
    "    \"electra\": (ElectraConfig, ElectraModel, ElectraTokenizer),\n",
    "    \"longformer\": (LongformerConfig, LongformerModel, LongformerTokenizer),\n",
    "    \"mobilebert\": (MobileBertConfig, MobileBertModel, MobileBertTokenizer),\n",
    "    \"marian\": (MarianConfig, MarianMTModel, MarianTokenizer),\n",
    "    \"roberta\": (RobertaConfig, RobertaModel, RobertaTokenizer),\n",
    "    \"gpt2\": (GPT2Model,GPT2Tokenizer),\n",
    "    \"mbart\": (MBartConfig, MBartForConditionalGeneration, MBart50TokenizerFast),\n",
    "    \"mgpt\": (GPT2Config, GPT2LMHeadModel ,GPT2Tokenizer)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Seq2SeqModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_type=None,\n",
    "        encoder_name=None,\n",
    "        decoder_name=None,\n",
    "        encoder_decoder_type=None,\n",
    "        encoder_decoder_name=None,\n",
    "        config=None,\n",
    "        args=None,\n",
    "        use_cuda=True,\n",
    "        cuda_device=0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a Seq2SeqModel.\n",
    "\n",
    "        Args:\n",
    "            encoder_type (optional): The type of model to use as the encoder.\n",
    "            encoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n",
    "            decoder_name (optional): The exact architecture and trained weights to use. This may be a Hugging Face Transformers compatible pre-trained model, a community model, or the path to a directory containing model files.\n",
    "                                    Must be the same \"size\" as the encoder model (base/base, large/large, etc.)\n",
    "            encoder_decoder_type (optional): The type of encoder-decoder model. (E.g. bart)\n",
    "            encoder_decoder_name (optional): The path to a directory containing the saved encoder and decoder of a Seq2SeqModel. (E.g. \"outputs/\") OR a valid BART or MarianMT model.\n",
    "            config (optional): A configuration file to build an EncoderDecoderModel.\n",
    "            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n",
    "            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n",
    "            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        if not config:\n",
    "            # if not ((encoder_name and decoder_name) or encoder_decoder_name) and not encoder_type:\n",
    "            if not ((encoder_name and decoder_name) or encoder_decoder_name):\n",
    "                raise ValueError(\n",
    "                    \"You must specify a Seq2Seq config \\t OR \\t\"\n",
    "                    \"encoder_type, encoder_name, and decoder_name OR \\t \\t\"\n",
    "                    \"encoder_type and encoder_decoder_name\"\n",
    "                )\n",
    "            elif not (encoder_type or encoder_decoder_type):\n",
    "                raise ValueError(\n",
    "                    \"You must specify a Seq2Seq config \\t OR \\t\"\n",
    "                    \"encoder_type, encoder_name, and decoder_name \\t OR \\t\"\n",
    "                    \"encoder_type and encoder_decoder_name\"\n",
    "                )\n",
    "\n",
    "        self.args = self._load_model_args(encoder_decoder_name)\n",
    "\n",
    "        if isinstance(args, dict):\n",
    "            self.args.update_from_dict(args)\n",
    "        elif isinstance(args, Seq2SeqArgs):\n",
    "            self.args = args\n",
    "\n",
    "        if \"sweep_config\" in kwargs:\n",
    "            sweep_config = kwargs.pop(\"sweep_config\")\n",
    "            sweep_values = {key: value[\"value\"] for key, value in sweep_config.as_dict().items() if key != \"_wandb\"}\n",
    "            self.args.update_from_dict(sweep_values)\n",
    "\n",
    "        if self.args.manual_seed:\n",
    "            random.seed(self.args.manual_seed)\n",
    "            np.random.seed(self.args.manual_seed)\n",
    "            torch.manual_seed(self.args.manual_seed)\n",
    "            if self.args.n_gpu > 0:\n",
    "                torch.cuda.manual_seed_all(self.args.manual_seed)\n",
    "\n",
    "        if use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                print(\"cuda is used\")\n",
    "                if cuda_device == -1:\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                else:\n",
    "                    self.device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'use_cuda' set to True when cuda is unavailable.\"\n",
    "                    \"Make sure CUDA is available or set `use_cuda=False`.\"\n",
    "                )\n",
    "        else:\n",
    "            print(\"cpu is used\")\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        self.encoder_tokenizer = None\n",
    "        self.decoder_tokenizer = None\n",
    "\n",
    "        if not use_cuda:\n",
    "            self.args.fp16 = False\n",
    "\n",
    "        # config = EncoderDecoderConfig.from_encoder_decoder_configs(config, config)\n",
    "        if encoder_decoder_type:\n",
    "            config_class, model_class, tokenizer_class = MODEL_CLASSES[encoder_decoder_type]\n",
    "        else:\n",
    "            config_class, model_class, tokenizer_class = MODEL_CLASSES[encoder_type]\n",
    "\n",
    "        if encoder_decoder_type in [\"bart\", \"marian\"]:\n",
    "            self.model = model_class.from_pretrained(encoder_decoder_name)\n",
    "            if encoder_decoder_type == \"bart\":\n",
    "                self.encoder_tokenizer = tokenizer_class.from_pretrained(encoder_decoder_name)\n",
    "            elif encoder_decoder_type == \"marian\":\n",
    "                if self.args.base_marian_model_name:\n",
    "                    self.encoder_tokenizer = tokenizer_class.from_pretrained(self.args.base_marian_model_name)\n",
    "                else:\n",
    "                    self.encoder_tokenizer = tokenizer_class.from_pretrained(encoder_decoder_name)\n",
    "            self.decoder_tokenizer = self.encoder_tokenizer\n",
    "            self.config = self.model.config\n",
    "        elif encoder_decoder_type == \"mbart\":\n",
    "            self.model = MBartForConditionalGeneration.from_pretrained(encoder_decoder_name)\n",
    "            self.encoder_tokenizer = MBart50TokenizerFast.from_pretrained(encoder_decoder_name)\n",
    "            self.decoder_tokenizer = self.encoder_tokenizer  \n",
    "        #elif encoder_decoder_type == \"mgpt\":\n",
    "        #        self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "        #            os.path.join(encoder_decoder_name, \"encoder\"), os.path.join(encoder_decoder_name, \"decoder\")\n",
    "        #        )    \n",
    "        #        self.model = GPT2LMHeadModel.from_pretrained(self.model)\n",
    "        #        self.decoder_tokenizer = GPT2Tokenizer.from_pretrained(self.model)\n",
    "        #        self.encoder_tokenizer = self.decoder_tokenizer\n",
    "        else:\n",
    "            if encoder_decoder_name:\n",
    "                # self.model = EncoderDecoderModel.from_pretrained(encoder_decoder_name)\n",
    "                self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "                    os.path.join(encoder_decoder_name, \"encoder\"), os.path.join(encoder_decoder_name, \"decoder\")\n",
    "                )\n",
    "                self.model.encoder = model_class.from_pretrained(os.path.join(encoder_decoder_name, \"encoder\"))\n",
    "                self.model.decoder = BertForMaskedLM.from_pretrained(os.path.join(encoder_decoder_name, \"decoder\"))\n",
    "                self.encoder_tokenizer = tokenizer_class.from_pretrained(os.path.join(encoder_decoder_name, \"encoder\"))\n",
    "                self.decoder_tokenizer = BertTokenizer.from_pretrained(os.path.join(encoder_decoder_name, \"decoder\"))\n",
    "            else:\n",
    "                self.model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "                    encoder_name, decoder_name, config=config\n",
    "                )\n",
    "                self.encoder_tokenizer = tokenizer_class.from_pretrained(encoder_name)\n",
    "                self.decoder_tokenizer = BertTokenizer.from_pretrained(decoder_name)\n",
    "            self.encoder_config = self.model.config.encoder\n",
    "            self.decoder_config = self.model.config.decoder\n",
    "\n",
    "        #print(\"encoder_tokenizer:\", self.encoder_tokenizer)\n",
    "        #print(\"decoder_tokenizer:\", self.decoder_tokenizer)    \n",
    "\n",
    "        if self.args.wandb_project and not wandb_available:\n",
    "            warnings.warn(\"wandb_project specified but wandb is not available. Wandb disabled.\")\n",
    "            self.args.wandb_project = None\n",
    "\n",
    "        if encoder_decoder_name:\n",
    "            self.args.model_name = encoder_decoder_name\n",
    "\n",
    "            # # Checking if we are loading from a saved model or using a pre-trained model\n",
    "            # if not saved_model_args and encoder_decoder_type == \"marian\":\n",
    "            # Need to store base pre-trained model name to get the tokenizer when loading a saved model\n",
    "            self.args.base_marian_model_name = encoder_decoder_name\n",
    "\n",
    "        elif encoder_name and decoder_name:\n",
    "            self.args.model_name = encoder_name + \"-\" + decoder_name\n",
    "        else:\n",
    "            self.args.model_name = \"encoder-decoder\"\n",
    "\n",
    "        if encoder_decoder_type:\n",
    "            self.args.model_type = encoder_decoder_type\n",
    "        elif encoder_type:\n",
    "            self.args.model_type = encoder_type + \"-bert\"\n",
    "        else:\n",
    "            self.args.model_type = \"encoder-decoder\"\n",
    "\n",
    "    def train_model(\n",
    "        self, train_data, best_accuracy, output_dir=None, show_running_loss=True, args=None, eval_data=None, verbose=True, **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the model using 'train_data'\n",
    "\n",
    "        Args:\n",
    "            train_data: Pandas DataFrame containing the 2 columns - `input_text`, `target_text`.\n",
    "                        - `input_text`: The input text sequence.\n",
    "                        - `target_text`: The target text sequence\n",
    "            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n",
    "            show_running_loss (optional): Set to False to prevent running loss from being printed to console. Defaults to True.\n",
    "            args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n",
    "            eval_data (optional): A DataFrame against which evaluation will be performed when evaluate_during_training is enabled. Is required if evaluate_during_training is enabled.\n",
    "            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n",
    "                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n",
    "                        will be lists of strings. Note that this will slow down training significantly as the predicted sequences need to be generated.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        if args:\n",
    "            self.args.update_from_dict(args)\n",
    "\n",
    "        # if self.args.silent:\n",
    "        #     show_running_loss = False\n",
    "\n",
    "        if self.args.evaluate_during_training and eval_data is None:\n",
    "            raise ValueError(\n",
    "                \"evaluate_during_training is enabled but eval_data is not specified.\"\n",
    "                \" Pass eval_data to model.train_model() if using evaluate_during_training.\"\n",
    "            )\n",
    "\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "\n",
    "        if os.path.exists(output_dir) and os.listdir(output_dir) and not self.args.overwrite_output_dir:\n",
    "            raise ValueError(\n",
    "                \"Output directory ({}) already exists and is not empty.\"\n",
    "                \" Set args.overwrite_output_dir = True to overcome.\".format(output_dir)\n",
    "            )\n",
    "\n",
    "        self._move_model_to_device()\n",
    "\n",
    "        if self.encoder_tokenizer is None or self.decoder_tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer initialization failed.\")\n",
    "\n",
    "        train_dataset = self.load_and_cache_examples(train_data, verbose=verbose)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        global_step, tr_loss, best_accuracy = self.train(\n",
    "            train_dataset,\n",
    "            output_dir,\n",
    "            best_accuracy,\n",
    "            show_running_loss=show_running_loss,\n",
    "            eval_data=eval_data,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._save_model(self.args.output_dir, model=self.model)\n",
    "\n",
    "        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        self.encoder_tokenizer.save_pretrained(output_dir)\n",
    "        self.decoder_tokenizer.save_pretrained(output_dir)\n",
    "        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "\n",
    "        if verbose:\n",
    "            logger.info(\" Training of {} model complete. Saved to {}.\".format(self.args.model_name, output_dir))\n",
    "\n",
    "        return best_accuracy\n",
    "\n",
    "    def train(\n",
    "        self, train_dataset, output_dir, best_accuracy, show_running_loss=True, eval_data=None, verbose=True, **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the model on train_dataset.\n",
    "\n",
    "        Utility function to be used by the train_model() method. Not intended to be used directly.\n",
    "        \"\"\"\n",
    "\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "\n",
    "        tb_writer = SummaryWriter(logdir=args.tensorboard_dir)\n",
    "        train_sampler = RandomSampler(train_dataset)\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler=train_sampler,\n",
    "            batch_size=args.train_batch_size,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "        )\n",
    "\n",
    "        if args.max_steps > 0:\n",
    "            t_total = args.max_steps\n",
    "            args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "        else:\n",
    "            t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "        optimizer_grouped_parameters = []\n",
    "        custom_parameter_names = set()\n",
    "        for group in self.args.custom_parameter_groups:\n",
    "            params = group.pop(\"params\")\n",
    "            custom_parameter_names.update(params)\n",
    "            param_group = {**group}\n",
    "            param_group[\"params\"] = [p for n, p in model.named_parameters() if n in params]\n",
    "            optimizer_grouped_parameters.append(param_group)\n",
    "\n",
    "        for group in self.args.custom_layer_parameters:\n",
    "            layer_number = group.pop(\"layer\")\n",
    "            layer = f\"layer.{layer_number}.\"\n",
    "            group_d = {**group}\n",
    "            group_nd = {**group}\n",
    "            group_nd[\"weight_decay\"] = 0.0\n",
    "            params_d = []\n",
    "            params_nd = []\n",
    "            for n, p in model.named_parameters():\n",
    "                if n not in custom_parameter_names and layer in n:\n",
    "                    if any(nd in n for nd in no_decay):\n",
    "                        params_nd.append(p)\n",
    "                    else:\n",
    "                        params_d.append(p)\n",
    "                    custom_parameter_names.add(n)\n",
    "            group_d[\"params\"] = params_d\n",
    "            group_nd[\"params\"] = params_nd\n",
    "\n",
    "            optimizer_grouped_parameters.append(group_d)\n",
    "            optimizer_grouped_parameters.append(group_nd)\n",
    "\n",
    "        if not self.args.train_custom_parameters_only:\n",
    "            optimizer_grouped_parameters.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"params\": [\n",
    "                            p\n",
    "                            for n, p in model.named_parameters()\n",
    "                            if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "                        ],\n",
    "                        \"weight_decay\": args.weight_decay,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [\n",
    "                            p\n",
    "                            for n, p in model.named_parameters()\n",
    "                            if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "                        ],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        warmup_steps = math.ceil(t_total * args.warmup_ratio)\n",
    "        args.warmup_steps = warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n",
    "\n",
    "        # TODO: Use custom optimizer like with BertSum?\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            args.model_name\n",
    "            and os.path.isfile(os.path.join(args.model_name, \"optimizer.pt\"))\n",
    "            and os.path.isfile(os.path.join(args.model_name, \"scheduler.pt\"))\n",
    "        ):\n",
    "            # Load in optimizer and scheduler states\n",
    "            optimizer.load_state_dict(torch.load(os.path.join(args.model_name, \"optimizer.pt\")))\n",
    "            scheduler.load_state_dict(torch.load(os.path.join(args.model_name, \"scheduler.pt\")))\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        logger.info(\" Training started\")\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        model.zero_grad()\n",
    "        train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.silent, mininterval=0)\n",
    "        epoch_number = 0\n",
    "        best_eval_metric = None\n",
    "        early_stopping_counter = 0\n",
    "        steps_trained_in_current_epoch = 0\n",
    "        epochs_trained = 0\n",
    "\n",
    "        if args.model_name and os.path.exists(args.model_name):\n",
    "            try:\n",
    "                # set global_step to gobal_step of last saved checkpoint from model path\n",
    "                checkpoint_suffix = args.model_name.split(\"/\")[-1].split(\"-\")\n",
    "                if len(checkpoint_suffix) > 2:\n",
    "                    checkpoint_suffix = checkpoint_suffix[1]\n",
    "                else:\n",
    "                    checkpoint_suffix = checkpoint_suffix[-1]\n",
    "                global_step = int(checkpoint_suffix)\n",
    "                epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
    "                steps_trained_in_current_epoch = global_step % (\n",
    "                    len(train_dataloader) // args.gradient_accumulation_steps\n",
    "                )\n",
    "\n",
    "                logger.info(\"   Continuing training from checkpoint, will skip to saved global_step\")\n",
    "                logger.info(\"   Continuing training from epoch %d\", epochs_trained)\n",
    "                logger.info(\"   Continuing training from global step %d\", global_step)\n",
    "                logger.info(\"   Will skip the first %d steps in the current epoch\", steps_trained_in_current_epoch)\n",
    "            except ValueError:\n",
    "                logger.info(\"   Starting fine-tuning.\")\n",
    "\n",
    "        if args.evaluate_during_training:\n",
    "            training_progress_scores = self._create_training_progress_scores(**kwargs)\n",
    "\n",
    "        if args.wandb_project:\n",
    "            wandb.init(project=args.wandb_project, config={**asdict(args)}, **args.wandb_kwargs)\n",
    "            wandb.watch(self.model)\n",
    "\n",
    "        if args.fp16:\n",
    "            from torch.cuda import amp\n",
    "\n",
    "            scaler = amp.GradScaler()\n",
    "\n",
    "        hist_graph = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "         }\n",
    "        predicted_values = []\n",
    "        real_values = []\n",
    "        model.train()\n",
    "        #eps = 1e-6\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        for current_epoch in train_iterator:\n",
    "            if epochs_trained > 0:\n",
    "                epochs_trained -= 1\n",
    "                continue\n",
    "            train_iterator.set_description(f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\")\n",
    "            batch_iterator = tqdm(\n",
    "                train_dataloader,\n",
    "                desc=f\"Running Epoch {epoch_number} of {args.num_train_epochs}\",\n",
    "                disable=args.silent,\n",
    "                mininterval=0,\n",
    "            )\n",
    "            for step, batch in enumerate(batch_iterator):\n",
    "                if steps_trained_in_current_epoch > 0:\n",
    "                    steps_trained_in_current_epoch -= 1\n",
    "                    continue\n",
    "                # batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "                inputs = self._get_inputs_dict(batch)        \n",
    "               # if any(torch.isnan(value).any() or torch.isinf(value).any() for value in inputs.values()):\n",
    "                #    print(\"NaN or Inf found in input tensor.\")\n",
    "                #    print(\"Inputs:\", inputs)\n",
    "                #    break  # Optional: Stop training after the first occurrence\n",
    "                #print(\"Inputs:\", inputs)\n",
    "                #if args.fp16:\n",
    "                #    with amp.autocast():\n",
    "                #        outputs = model(**inputs)\n",
    "                    # Check for NaN in the model output\n",
    "                #        if torch.isnan(outputs[0]).any() or torch.isinf(outputs[0]).any():\n",
    "                #            print(\"NaN or Inf found in model output.\")\n",
    "                #            print(\"Outputs:\", outputs)\n",
    "                #            break  # Optional: Stop training after the first occurrence\n",
    "                #        loss = outputs[0]\n",
    "                #else:\n",
    "                #    outputs = model(**inputs)\n",
    "                    # Check for NaN in the model output\n",
    "                #    print(\"Outputs:\", outputs)\n",
    "                #    if torch.isnan(outputs[0]).any() or torch.isinf(outputs[0]).any():\n",
    "                #        print(\"NaN or Inf found in model output.\")\n",
    "                #        print(\"Outputs:\", outputs)\n",
    "                #        break  # Optional: Stop training after the first occurrence\n",
    "                #    loss = outputs[0]    \n",
    "                #print(input)\n",
    "                #print(\"\\n\")\n",
    "                #for key, value in inputs.items():\n",
    "                #    if torch.is_tensor(value) and (torch.isnan(value).any() or torch.isinf(value).any()):\n",
    "                #        print(f\"Input data for key '{key}' contains NaN or infinite values.\")\n",
    "                #        print(\"Inputs:\", value)\n",
    "                #        break  # Optional: Stop training after the first occurrence\n",
    "\n",
    "                #if torch.is_tensor(inputs.get(\"labels\")) and (torch.isnan(inputs[\"labels\"]).any() or torch.isinf(inputs[\"labels\"]).any()):\n",
    "                #    print(\"Labels data contains NaN or infinite values.\")\n",
    "                #    print(\"Labels:\", inputs[\"labels\"])\n",
    "                #    break  # Optional: Stop training after the first occurrence\n",
    "\n",
    "            #    for key, value in inputs.items():\n",
    "            #        print(f\"Key: {key}, NaN: {torch.isnan(value).any()}, Inf: {torch.isinf(value).any()}\\n\")\n",
    "            #    for param in model.parameters():\n",
    "            #        print(param.grad)\n",
    "            #    print(\"Softmax input:\", your_softmax_input)\n",
    "            #    if args.n_gpu > 1:\n",
    "            #        loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "                #current_loss = loss.item()\n",
    "                #print(\"Loss:\", current_loss)\n",
    "        #        if args.fp16:\n",
    "        #            with amp.autocast():\n",
    "        #                print(inputs)\n",
    "        #                outputs = model(**inputs)\n",
    "        #                print(outputs)\n",
    "        #                # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "        #                loss = outputs[0]\n",
    "        #        else:\n",
    "        #            print(inputs)\n",
    "        #            outputs = model(**inputs)\n",
    "        #            print(outputs)\n",
    "        #            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "        #            loss = outputs[0]\n",
    "                #if args.fp16:\n",
    "                #    with amp.autocast():\n",
    "                    # Check for NaN or infinite values in input tensors\n",
    "                #        for key, value in inputs.items():\n",
    "                #            if torch.isnan(value).any() or torch.isinf(value).any():\n",
    "                #                print(f\"Input tensor '{key}' contains NaN or infinite values.\")\n",
    "                #                break  # Stop processing further if NaN or infinite values are found\n",
    "                #            else:  # If no NaN or infinite values are found in any input tensor\n",
    "                #                print(\"No NaN or infinite values found in input tensors.\")\n",
    "                #    outputs = model(**inputs)\n",
    "                #    loss = outputs[0]\n",
    "                #else:\n",
    "                    # Check for NaN or infinite values in input tensors\n",
    "                #    for key, value in inputs.items():\n",
    "                #        if torch.isnan(value).any() or torch.isinf(value).any():\n",
    "                #            print(f\"Input tensor '{key}' contains NaN or infinite values.\")\n",
    "                #            break  # Stop processing further if NaN or infinite values are found\n",
    "                #        else:  # If no NaN or infinite values are found in any input tensor\n",
    "                #            print(\"No NaN or infinite values found in input tensors.\")\n",
    "                \n",
    "                outputs = model(**inputs)\n",
    "                #print(outputs)\n",
    "                loss = outputs[0]    \n",
    "\n",
    "                logits = outputs.logits\n",
    "                predicted_token_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "                input_token_ids = inputs[\"input_ids\"]\n",
    "                input_sentences = []\n",
    "                for token_ids in input_token_ids:\n",
    "                    sentence = self.encoder_tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                    input_sentences.append(sentence)\n",
    "\n",
    "                predicted_sentences = []\n",
    "                for token_ids in predicted_token_ids:\n",
    "                    sentence = self.decoder_tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "                    predicted_sentences.append(sentence)\n",
    "\n",
    "                for input_sentence, predicted_sentence in zip(input_sentences, predicted_sentences):\n",
    "                    print(\"Input Sentence:\", input_sentence)\n",
    "                    print(\"Predicted Sentence:\", predicted_sentence)\n",
    "                    print()  # Add a newline for better readability \n",
    "                print(\"\\n\")    \n",
    "        #        if args.n_gpu > 1:\n",
    "        #            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        #        if loss.isnan(): \n",
    "        #            current_loss=eps\n",
    "        #        else:\n",
    "        #            current_loss = loss.item()\n",
    "                current_loss = loss.item()\n",
    "        #        print(loss)\n",
    "                #print(\"\\n\")\n",
    "                if show_running_loss:\n",
    "                    batch_iterator.set_description(\n",
    "                        f\"Epochs {epoch_number}/{args.num_train_epochs}. Running Loss: {current_loss:9.4f}\"\n",
    "                    )\n",
    "\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "                if args.fp16:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    #torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                #optimizer.step()\n",
    "                #model.eval()\n",
    "                #with torch.no_grad():\n",
    "                #    input_ids = inputs[\"input_ids\"]\n",
    "                #   output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)[0]\n",
    "                #    logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "\n",
    "                #    predicted_values.extend(logits)\n",
    "                #    real_values.extend(inputs[\"labels\"])\n",
    "                #model.train()\n",
    "                #print(\"Loss:\", loss.item())\n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    if args.fp16:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                    if args.fp16:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        optimizer.step()   \n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1                \n",
    "                \n",
    "\n",
    "\n",
    "                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                        # Log metrics\n",
    "                        tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                        tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
    "                        logging_loss = tr_loss\n",
    "                        if args.wandb_project:\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"Training loss\": current_loss,\n",
    "                                    \"lr\": scheduler.get_lr()[0],\n",
    "                                    \"global_step\": global_step,\n",
    "                                }\n",
    "                            )\n",
    "\n",
    "                    if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                        # Save model checkpoint\n",
    "                        output_dir_current = os.path.join(output_dir, \"checkpoint-{}\".format(global_step))\n",
    "\n",
    "                        self._save_model(output_dir_current, optimizer, scheduler, model=model)\n",
    "\n",
    "                    if args.evaluate_during_training and (\n",
    "                        args.evaluate_during_training_steps > 0\n",
    "                        and global_step % args.evaluate_during_training_steps == 0\n",
    "                    ):\n",
    "                        # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = self.eval_model(\n",
    "                            eval_data,\n",
    "                            verbose=verbose and args.evaluate_during_training_verbose,\n",
    "                            silent=args.evaluate_during_training_silent,\n",
    "                            **kwargs,\n",
    "                        )\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "\n",
    "                        output_dir_current = os.path.join(output_dir, \"checkpoint-{}\".format(global_step))\n",
    "\n",
    "                        if args.save_eval_checkpoints:\n",
    "                            self._save_model(output_dir_current, optimizer, scheduler, model=model, results=results)\n",
    "\n",
    "                        training_progress_scores[\"global_step\"].append(global_step)\n",
    "                        training_progress_scores[\"train_loss\"].append(current_loss)\n",
    "                        for key in results:\n",
    "                            training_progress_scores[key].append(results[key])\n",
    "                        report = pd.DataFrame(training_progress_scores)\n",
    "                        report.to_csv(\n",
    "                            os.path.join(args.output_dir, \"training_progress_scores.csv\"), index=False,\n",
    "                        )\n",
    "\n",
    "                        if args.wandb_project:\n",
    "                            wandb.log(self._get_last_metrics(training_progress_scores))\n",
    "\n",
    "                        if not best_eval_metric:\n",
    "                            best_eval_metric = results[args.early_stopping_metric]\n",
    "                            if args.save_best_model:\n",
    "                                self._save_model(\n",
    "                                    args.best_model_dir, optimizer, scheduler, model=model, results=results\n",
    "                                )\n",
    "                        if best_eval_metric and args.early_stopping_metric_minimize:\n",
    "                            if results[args.early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n",
    "                                best_eval_metric = results[args.early_stopping_metric]\n",
    "                                if args.save_best_model:\n",
    "                                    self._save_model(\n",
    "                                        args.best_model_dir, optimizer, scheduler, model=model, results=results\n",
    "                                    )\n",
    "                                early_stopping_counter = 0\n",
    "                            else:\n",
    "                                if args.use_early_stopping:\n",
    "                                    if early_stopping_counter < args.early_stopping_patience:\n",
    "                                        early_stopping_counter += 1\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                            logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                            logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                                    else:\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                            logger.info(\" Training terminated.\")\n",
    "                                            train_iterator.close()\n",
    "                                        return global_step, tr_loss / global_step\n",
    "                        else:\n",
    "                            if results[args.early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n",
    "                                best_eval_metric = results[args.early_stopping_metric]\n",
    "                                if args.save_best_model:\n",
    "                                    self._save_model(\n",
    "                                        args.best_model_dir, optimizer, scheduler, model=model, results=results\n",
    "                                    )\n",
    "                                early_stopping_counter = 0\n",
    "                            else:\n",
    "                                if args.use_early_stopping:\n",
    "                                    if early_stopping_counter < args.early_stopping_patience:\n",
    "                                        early_stopping_counter += 1\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                            logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                            logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                                    else:\n",
    "                                        if verbose:\n",
    "                                            logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                            logger.info(\" Training terminated.\")\n",
    "                                            train_iterator.close()\n",
    "                                        return global_step, tr_loss / global_step\n",
    "                                        \n",
    "\n",
    "            epoch_number += 1\n",
    "            output_dir_current = os.path.join(output_dir, \"checkpoint-{}-epoch-{}\".format(global_step, epoch_number))\n",
    "\n",
    "            #train_accuracy = accuracy_score(real_values, predicted_values)\n",
    "            #train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(real_values, predicted_values, average='macro', zero_division=1.0)\n",
    "            val_accuracy,val_precision, val_recall, val_f1_score = predict_val(model, self.device)#, output_dir_current)\n",
    "            print('batch: '+str(args.train_batch_size)+' accumulation_steps: '+str(args.gradient_accumulation_steps)+\\\n",
    "                ' lr: '+str(args.learning_rate)+' epochs: '+str(args.num_train_epochs)+' epoch: '+str(epoch_number))\n",
    "            #if val_accuracy > best_accuracy:\n",
    "            #    best_accuracy = val_accuracy\n",
    "            #    print('---test dataset----')\n",
    "            #    test_acc,test_precision, test_recall, test_f1_score = predict_test(model, self.device)#, output_dir_current)\n",
    "            #    with open('/ICTeval/ICTeval_single/courseEval250/output.txt', 'a') as f0:\n",
    "            #        f0.writelines('batch: '+str(args.train_batch_size)+' accumulation_steps: '+str(args.gradient_accumulation_steps)+\\\n",
    "            #                      ' lr: '+str(args.learning_rate)+' epochs: '+str(args.num_train_epochs)+' epoch: '+str(epoch_number)+' val_accuracy: '+str(best_accuracy)+\\\n",
    "            #                      ' val_precision: ' +str(val_precision) +' val_recall: '  +str(val_recall) +' val_f1_score: ' +str(val_f1_score) +' test_accuracy: '+str(test_acc)+\\\n",
    "            #                      ' test_precision: ' +str(test_precision) +' test_recall: ' +str(test_recall) +' test_f1_score: ' +str(test_f1_score)+'\\n')\n",
    "            #hist_graph['train_loss'].append(tr_loss)\n",
    "            #hist_graph['train_acc'].append(train_accuracy)\n",
    "            #hist_graph['train_precision'].append(train_precision)\n",
    "            #hist_graph['train_recall'].append(train_recall)\n",
    "            #hist_graph['train_f1'].append(train_f1)\n",
    "            #hist_graph['val_loss'].append(val_avg_loss)\n",
    "\n",
    "            #fig, ax = plt.subplots(figsize=(8,6))\n",
    "            #ax.plot(hist_graph['train_loss'], label='train_loss')\n",
    "            #ax.plot(hist_graph['val_loss'], label='val_loss')\n",
    "            #ax.set_ylabel('Loss')\n",
    "            #ax.set_xlabel('Epochs')\n",
    "            #plt.legend()\n",
    "            #plt.savefig('graph_loss.png')\n",
    "\n",
    "            if args.save_model_every_epoch or args.evaluate_during_training:\n",
    "                os.makedirs(output_dir_current, exist_ok=True)\n",
    "\n",
    "            if args.save_model_every_epoch:\n",
    "                self._save_model(output_dir_current, optimizer, scheduler, model=model)\n",
    "\n",
    "            if args.evaluate_during_training:\n",
    "                results = self.eval_model(\n",
    "                    eval_data,\n",
    "                    verbose=verbose and args.evaluate_during_training_verbose,\n",
    "                    silent=args.evaluate_during_training_silent,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "                if args.save_eval_checkpoints:\n",
    "                    self._save_model(output_dir_current, optimizer, scheduler, results=results)\n",
    "\n",
    "                training_progress_scores[\"global_step\"].append(global_step)\n",
    "                training_progress_scores[\"train_loss\"].append(current_loss)\n",
    "                for key in results:\n",
    "                    training_progress_scores[key].append(results[key])\n",
    "                report = pd.DataFrame(training_progress_scores)\n",
    "                report.to_csv(os.path.join(args.output_dir, \"training_progress_scores.csv\"), index=False)\n",
    "\n",
    "                if args.wandb_project:\n",
    "                    wandb.log(self._get_last_metrics(training_progress_scores))\n",
    "\n",
    "                if not best_eval_metric:\n",
    "                    best_eval_metric = results[args.early_stopping_metric]\n",
    "                    if args.save_best_model:\n",
    "                        self._save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n",
    "                if best_eval_metric and args.early_stopping_metric_minimize:\n",
    "                    if results[args.early_stopping_metric] - best_eval_metric < args.early_stopping_delta:\n",
    "                        best_eval_metric = results[args.early_stopping_metric]\n",
    "                        if args.save_best_model:\n",
    "                            self._save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n",
    "                            if early_stopping_counter < args.early_stopping_patience:\n",
    "                                early_stopping_counter += 1\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                    logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                    logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                            else:\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                    logger.info(\" Training terminated.\")\n",
    "                                    train_iterator.close()\n",
    "                                return global_step, tr_loss / global_step\n",
    "                else:\n",
    "                    if results[args.early_stopping_metric] - best_eval_metric > args.early_stopping_delta:\n",
    "                        best_eval_metric = results[args.early_stopping_metric]\n",
    "                        if args.save_best_model:\n",
    "                            self._save_model(args.best_model_dir, optimizer, scheduler, model=model, results=results)\n",
    "                        early_stopping_counter = 0\n",
    "                    else:\n",
    "                        if args.use_early_stopping and args.early_stopping_consider_epochs:\n",
    "                            if early_stopping_counter < args.early_stopping_patience:\n",
    "                                early_stopping_counter += 1\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" No improvement in {args.early_stopping_metric}\")\n",
    "                                    logger.info(f\" Current step: {early_stopping_counter}\")\n",
    "                                    logger.info(f\" Early stopping patience: {args.early_stopping_patience}\")\n",
    "                            else:\n",
    "                                if verbose:\n",
    "                                    logger.info(f\" Patience of {args.early_stopping_patience} steps reached\")\n",
    "                                    logger.info(\" Training terminated.\")\n",
    "                                    train_iterator.close()\n",
    "                                return global_step, tr_loss / global_step\n",
    "\n",
    "        return global_step, tr_loss / global_step, best_accuracy\n",
    "\n",
    "    def eval_model(self, eval_data, output_dir=None, verbose=True, silent=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluates the model on eval_data. Saves results to output_dir.\n",
    "\n",
    "        Args:\n",
    "            eval_data: Pandas DataFrame containing the 2 columns - `input_text`, `target_text`.\n",
    "                        - `input_text`: The input text sequence.\n",
    "                        - `target_text`: The target text sequence.\n",
    "            output_dir: The directory where model files will be saved. If not given, self.args.output_dir will be used.\n",
    "            verbose: If verbose, results will be printed to the console on completion of evaluation.\n",
    "            silent: If silent, tqdm progress bars will be hidden.\n",
    "            **kwargs: Additional metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n",
    "                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n",
    "                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n",
    "        Returns:\n",
    "            results: Dictionary containing evaluation results.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "\n",
    "        self._move_model_to_device()\n",
    "\n",
    "        eval_dataset = self.load_and_cache_examples(eval_data, evaluate=True, verbose=verbose, silent=silent)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        result = self.evaluate(eval_dataset, output_dir, verbose=verbose, silent=silent, **kwargs)\n",
    "        self.results.update(result)\n",
    "\n",
    "        if self.args.evaluate_generated_text:\n",
    "            to_predict = eval_data[\"input_text\"].tolist()\n",
    "            preds = self.predict(to_predict)\n",
    "\n",
    "            result = self.compute_metrics(eval_data[\"target_text\"].tolist(), preds, **kwargs)\n",
    "            self.results.update(result)\n",
    "\n",
    "        if verbose:\n",
    "            logger.info(self.results)\n",
    "\n",
    "        return self.results\n",
    "\n",
    "    def evaluate(self, eval_dataset, output_dir, verbose=True, silent=False, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluates the model on eval_dataset.\n",
    "\n",
    "        Utility function to be used by the eval_model() method. Not intended to be used directly.\n",
    "        \"\"\"\n",
    "\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "        eval_output_dir = output_dir\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        eval_sampler = SequentialSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        model.eval()\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, disable=args.silent or silent, desc=\"Running Evaluation\"):\n",
    "            # batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            inputs = self._get_inputs_dict(batch)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs[0]\n",
    "                eval_loss += loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "        results[\"eval_loss\"] = eval_loss\n",
    "\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            for key in sorted(results.keys()):\n",
    "                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def predict(self, to_predict):\n",
    "        \"\"\"\n",
    "        Performs predictions on a list of text.\n",
    "\n",
    "        Args:\n",
    "            to_predict: A python list of text (str) to be sent to the model for prediction. Note that the prefix should be prepended to the text.\n",
    "\n",
    "        Returns:\n",
    "            preds: A python list of the generated sequences.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        self._move_model_to_device()\n",
    "\n",
    "        all_outputs = []\n",
    "        # Batching\n",
    "        for batch in [\n",
    "            to_predict[i : i + self.args.eval_batch_size] for i in range(0, len(to_predict), self.args.eval_batch_size)\n",
    "        ]:\n",
    "            if self.args.model_type == \"marian\":\n",
    "                input_ids = self.encoder_tokenizer.prepare_translation_batch(\n",
    "                    batch, max_length=self.args.max_seq_length, pad_to_max_length=True, return_tensors=\"pt\",\n",
    "                )[\"input_ids\"]\n",
    "            else:\n",
    "                input_ids = self.encoder_tokenizer.batch_encode_plus(\n",
    "                    batch, max_length=self.args.max_seq_length, pad_to_max_length=True, return_tensors=\"pt\",\n",
    "                )[\"input_ids\"]\n",
    "            input_ids = input_ids.to(self.device)\n",
    "\n",
    "            if self.args.model_type in [\"bart\", \"marian\",\"mbart\"]:\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    num_beams=self.args.num_beams,\n",
    "                    max_length=self.args.max_length,\n",
    "                    length_penalty=self.args.length_penalty,\n",
    "                    early_stopping=self.args.early_stopping,\n",
    "                    repetition_penalty=self.args.repetition_penalty,\n",
    "                    do_sample=self.args.do_sample,\n",
    "                    top_k=self.args.top_k,\n",
    "                    top_p=self.args.top_p,\n",
    "                    num_return_sequences=self.args.num_return_sequences,\n",
    "                )\n",
    "            else:\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    decoder_start_token_id=self.model.config.decoder.pad_token_id,\n",
    "                    num_beams=self.args.num_beams,\n",
    "                    max_length=self.args.max_length,\n",
    "                    length_penalty=self.args.length_penalty,\n",
    "                    early_stopping=self.args.early_stopping,\n",
    "                    repetition_penalty=self.args.repetition_penalty,\n",
    "                    do_sample=self.args.do_sample,\n",
    "                    top_k=self.args.top_k,\n",
    "                    top_p=self.args.top_p,\n",
    "                    num_return_sequences=self.args.num_return_sequences,\n",
    "                )\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "\n",
    "        if self.args.use_multiprocessed_decoding:\n",
    "            self.model.to(\"cpu\")\n",
    "            with Pool(self.args.process_count) as p:\n",
    "                outputs = list(\n",
    "                    tqdm(\n",
    "                        p.imap(self._decode, all_outputs, chunksize=self.args.multiprocessing_chunksize),\n",
    "                        total=len(all_outputs),\n",
    "                        desc=\"Decoding outputs\",\n",
    "                        disable=self.args.silent,\n",
    "                    )\n",
    "                )\n",
    "            self._move_model_to_device()\n",
    "        else:\n",
    "            outputs = [\n",
    "                self.decoder_tokenizer.decode(output_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "                for output_id in all_outputs\n",
    "            ]\n",
    "\n",
    "        if self.args.num_return_sequences > 1:\n",
    "            return [\n",
    "                outputs[i : i + self.args.num_return_sequences]\n",
    "                for i in range(0, len(outputs), self.args.num_return_sequences)\n",
    "            ]\n",
    "        else:\n",
    "            return outputs\n",
    "\n",
    "    def _decode(self, output_id):\n",
    "        return self.decoder_tokenizer.decode(output_id, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    def compute_metrics(self, labels, preds, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the evaluation metrics for the model predictions.\n",
    "\n",
    "        Args:\n",
    "            labels: List of target sequences\n",
    "            preds: List of model generated outputs\n",
    "            **kwargs: Custom metrics that should be used. Pass in the metrics as keyword arguments (name of metric: function to use).\n",
    "                        A metric function should take in two parameters. The first parameter will be the true labels, and the second parameter will be the predictions. Both inputs\n",
    "                        will be lists of strings. Note that this will slow down evaluation significantly as the predicted sequences need to be generated.\n",
    "\n",
    "        Returns:\n",
    "            result: Dictionary containing evaluation results.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "        # assert len(labels) == len(preds)\n",
    "\n",
    "        results = {}\n",
    "        for metric, func in kwargs.items():\n",
    "            results[metric] = func(labels, preds)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def load_and_cache_examples(self, data, evaluate=False, no_cache=False, verbose=True, silent=False):\n",
    "        \"\"\"\n",
    "        Creates a T5Dataset from data.\n",
    "\n",
    "        Utility function for train() and eval() methods. Not intended to be used directly.\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_tokenizer = self.encoder_tokenizer\n",
    "        decoder_tokenizer = self.decoder_tokenizer\n",
    "        args = self.args\n",
    "\n",
    "        if not no_cache:\n",
    "            no_cache = args.no_cache\n",
    "\n",
    "        if not no_cache:\n",
    "            os.makedirs(self.args.cache_dir, exist_ok=True)\n",
    "\n",
    "        mode = \"dev\" if evaluate else \"train\"\n",
    "\n",
    "        if args.dataset_class:\n",
    "            CustomDataset = args.dataset_class\n",
    "            return CustomDataset(encoder_tokenizer, decoder_tokenizer, args, data, mode)\n",
    "        else:\n",
    "            if args.model_type in [\"bart\", \"marian\"]:\n",
    "                return SimpleSummarizationDataset(encoder_tokenizer, self.args, data, mode)\n",
    "            else:\n",
    "                return Seq2SeqDataset(encoder_tokenizer, decoder_tokenizer, self.args, data, mode,)\n",
    "\n",
    "    def _create_training_progress_scores(self, **kwargs):\n",
    "        extra_metrics = {key: [] for key in kwargs}\n",
    "        training_progress_scores = {\n",
    "            \"global_step\": [],\n",
    "            \"eval_loss\": [],\n",
    "            \"train_loss\": [],\n",
    "            **extra_metrics,\n",
    "        }\n",
    "\n",
    "        return training_progress_scores\n",
    "\n",
    "    def _get_last_metrics(self, metric_values):\n",
    "        return {metric: values[-1] for metric, values in metric_values.items()}\n",
    "\n",
    "    def _save_model(self, output_dir=None, optimizer=None, scheduler=None, model=None, results=None):\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        logger.info(f\"Saving model into {output_dir}\")\n",
    "\n",
    "        if model and not self.args.no_save:\n",
    "            # Take care of distributed/parallel training\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            self._save_model_args(output_dir)\n",
    "\n",
    "            if self.args.model_type in [\"bart\", \"marian\"]:\n",
    "                os.makedirs(os.path.join(output_dir), exist_ok=True)\n",
    "                model_to_save.save_pretrained(output_dir)\n",
    "                self.config.save_pretrained(output_dir)\n",
    "                if self.args.model_type == \"bart\":\n",
    "                    self.encoder_tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            elif self.args.model_type == \"mbart\":\n",
    "                os.makedirs(os.path.join(output_dir, \"mbart\"), exist_ok=True)\n",
    "                model_to_save.save_pretrained(os.path.join(output_dir, \"mbart\"))\n",
    "                self.encoder_tokenizer.save_pretrained(os.path.join(output_dir, \"mbart\"))\n",
    "            else:\n",
    "                os.makedirs(os.path.join(output_dir, \"encoder\"), exist_ok=True)\n",
    "                os.makedirs(os.path.join(output_dir, \"decoder\"), exist_ok=True)\n",
    "\n",
    "                self.encoder_config.save_pretrained(os.path.join(output_dir, \"encoder\"))\n",
    "                self.decoder_config.save_pretrained(os.path.join(output_dir, \"decoder\"))\n",
    "\n",
    "                model_to_save = (\n",
    "                    self.model.encoder.module if hasattr(self.model.encoder, \"module\") else self.model.encoder\n",
    "                )\n",
    "                model_to_save.save_pretrained(os.path.join(output_dir, \"encoder\"))\n",
    "\n",
    "                model_to_save = (\n",
    "                    self.model.decoder.module if hasattr(self.model.decoder, \"module\") else self.model.decoder\n",
    "                )\n",
    "\n",
    "                model_to_save.save_pretrained(os.path.join(output_dir, \"decoder\"))\n",
    "\n",
    "                self.encoder_tokenizer.save_pretrained(os.path.join(output_dir, \"encoder\"))\n",
    "                self.decoder_tokenizer.save_pretrained(os.path.join(output_dir, \"decoder\"))\n",
    "\n",
    "            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            if optimizer and scheduler and self.args.save_optimizer_and_scheduler:\n",
    "                torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "\n",
    "        if results:\n",
    "            output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                for key in sorted(results.keys()):\n",
    "                    writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "\n",
    "    def _move_model_to_device(self):\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _get_inputs_dict(self, batch):\n",
    "        device = self.device\n",
    "        if self.args.model_type in [\"bart\", \"marian\"]:\n",
    "            pad_token_id = self.decoder_tokenizer.pad_token_id\n",
    "            source_ids, source_mask, y = batch[\"source_ids\"], batch[\"source_mask\"], batch[\"target_ids\"]\n",
    "            y_ids = y[:, :-1].contiguous()\n",
    "            labels = y[:, 1:].clone()\n",
    "            labels[y[:, 1:] == pad_token_id] = -100\n",
    "\n",
    "            inputs = {\n",
    "                \"input_ids\": source_ids.to(device),\n",
    "                \"attention_mask\": source_mask.to(device),\n",
    "                \"decoder_input_ids\": y_ids.to(device),\n",
    "                \"labels\": labels.to(device),\n",
    "            }\n",
    "        else:\n",
    "            labels = batch[1]\n",
    "            labels_masked = labels.clone()\n",
    "            labels_masked[labels_masked == self.decoder_tokenizer.pad_token_id] = -100\n",
    "\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0].to(device),\n",
    "                \"decoder_input_ids\": labels.to(device),\n",
    "                \"labels\": labels_masked.to(device),\n",
    "            }\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _save_model_args(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.args.save(output_dir)\n",
    "\n",
    "    def _load_model_args(self, input_dir):\n",
    "        args = Seq2SeqArgs()\n",
    "        args.load(input_dir)\n",
    "        return args\n",
    "\n",
    "    def get_named_parameters(self):\n",
    "        return [n for n, p in self.model.named_parameters()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
