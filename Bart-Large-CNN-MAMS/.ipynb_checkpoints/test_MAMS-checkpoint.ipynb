{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'course-eval/Bart-Large-CNN'\n",
      "/home/thanapon.nor/course-eval/Bart-Large-CNN\n"
     ]
    }
   ],
   "source": [
    "%cd course-eval/Bart-Large-CNN\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34m__pycache__\u001b[0m/        \u001b[01;34mruns\u001b[0m/                  train.ipynb\n",
      "bart-large-cnn.zip  seq2seq_model_M.ipynb  train_MAMS_sentiment.ipynb\n",
      "\u001b[01;34mcache_dir\u001b[0m/          seq2seq_model_M.py     train_MAMS_sentiment.py\n",
      "cp-remover.ipynb    test_MAMS.ipynb\n",
      "\u001b[01;34moutputs\u001b[0m/            test_MAMS.py\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from simpletransformers.seq2seq import Seq2SeqModel\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support, log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig, GPT2Tokenizer, GPT2Model, MBart50TokenizerFast, MBartForConditionalGeneration, GPT2LMHeadModel\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# transformers_logger = logging.getLogger(\"transformers\")\n",
    "# transformers_logger.setLevel(logging.WARNING)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Path: ../Dataset/Original/Splited/1-fold/testformat.txt\n",
      "Model Path: /outputs/Original/1-fold\n"
     ]
    }
   ],
   "source": [
    "fold = \"1\"\n",
    "\n",
    "# Original, Back-Translation\n",
    "model_type = \"Original\"\n",
    "\n",
    "path = f\"../Dataset/{model_type}/Splited/{fold}-fold/testformat.txt\"\n",
    "model_path = f\"/outputs/{model_type}/{fold}-fold\"\n",
    "\n",
    "print(f\"Test Path: {path}\\nModel Path: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA devices available: 1\n",
      "Current CUDA device index: 0\n",
      "Current CUDA device name: NVIDIA RTX A6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"CUDA devices available: {torch.cuda.device_count()}\")\n",
    "#     print(f\"Current CUDA device index: {torch.cuda.current_device()}\")\n",
    "#     print(f\"Current CUDA device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "# else:\n",
    "#     print(\"CUDA is not available. Switch to CPU mode or check your CUDA installation.\")\n",
    "\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#def calculate_metrics(predicted, golden):\n",
    "#    label_mapping = {\"positive\": 1, \"neutral\": 2, \"negative\": 0}\n",
    "\n",
    "    # Map labels to numeric values and handle 'nan' values\n",
    "#    golden_numeric = np.array([label_mapping.get(label, -1) for label in golden])\n",
    "\n",
    "    # Filter out entries with -1 (nan) before further calculations\n",
    "#    valid_entries = golden_numeric != -1\n",
    "#    predicted_numeric = np.array([label_mapping[label] for label in predicted])[valid_entries]\n",
    "#    golden_numeric = golden_numeric[valid_entries]\n",
    "\n",
    "#    TP = np.sum(np.equal(predicted_numeric, golden_numeric) & np.equal(predicted_numeric, 1))\n",
    "#    FP = np.sum(np.not_equal(predicted_numeric, golden_numeric) & np.equal(predicted_numeric, 1))\n",
    "#    FN = np.sum(np.not_equal(predicted_numeric, golden_numeric) & np.equal(predicted_numeric, 0))\n",
    "#    TN = np.sum(np.equal(predicted_numeric, golden_numeric) & np.equal(predicted_numeric, 0))\n",
    "\n",
    "#    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "#    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "#    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "#    return precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_val(model, device):#, output_dir):\n",
    "    candidate_list = [ \"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "    #model = MBartForConditionalGeneration.from_pretrained(output_dir)\n",
    "    #model = MBartForConditionalGeneration.from_pretrained('/ICTeval/ICTeval_code/outputs/mbart')\n",
    "    model.eval()\n",
    "    model.config.use_cache = False\n",
    "    #tokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50')\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    #tokenizer = GPT2Tokenizer.from_pretrained(\"sberbank-ai/mGPT\")\n",
    "    with open(path, \"r\") as f:\n",
    "        file = f.readlines()\n",
    "    train_data = []\n",
    "    count = 0\n",
    "    total = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "   #with torch.no_grad():\n",
    "   #     for module in model.modules():\n",
    "   #         if isinstance(module, nn.Dropout):\n",
    "   #             module.training = False\n",
    "   #         elif isinstance(module, nn.BatchNorm1d):\n",
    "   #             module.eval()\n",
    "\n",
    "    for line in file:\n",
    "        total += 1\n",
    "        # score_list = []\n",
    "        score_list1 = []\n",
    "        score_list2 = []\n",
    "        score_list3 = []\n",
    "        score_list4 = []\n",
    "        score_list5 = []\n",
    "        line = line.strip()\n",
    "        x, term, golden_polarity = line.split(\"\\001\")[0], line.split(\"\\001\")[1], line.split(\"\\001\")[2]\n",
    "        input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids'].to(device)\n",
    "\n",
    "        # target_list = [\"For \" + term.lower() + \", the sentiment is \" + candi.lower() + \" .\" for candi in candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list1.append(score)\n",
    "\n",
    "        target_list = [\"The sentiment polarity of \" + term.lower() + \" is \" + candi.lower() + \" .\" for candi in\n",
    "                       candidate_list]\n",
    "        ## target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        ## input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "\n",
    "        output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n",
    "        model.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, decoder_input_ids=output_ids)[0]\n",
    "            logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        list_score = []\n",
    "        #with torch.no_grad():\n",
    "        #    output = model(input_ids=input_ids.to(device))[0]\n",
    "        #    logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        for i in range(3):\n",
    "            score = 0\n",
    "            for j in range(logits[i].shape[0] - 2):\n",
    "                #print(logits[i][j][output_ids[i][j + 1]])\n",
    "                score += np.log(logits[i][j][output_ids[i][j + 1]])\n",
    "                list_score.append(np.log(logits[i][j][output_ids[i][j + 1]]))\n",
    "            score_list2.append(score)\n",
    "        score_list = score_list2\n",
    "        #print(\"log lsit:\",np.array(list_score).reshape(3,-1))\n",
    "\n",
    "        #loss = -torch.log(torch.tensor(score_list)).mean().item()  # Calculate loss for the example\n",
    "        #losses.append(loss)\n",
    "\n",
    "        # target_list = [\"The \" + term.lower() + \" category has a \" + candi.lower() + \" label .\" for candi in\n",
    "        #                candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list3.append(score)\n",
    "\n",
    "        # target_list = [\"The sentiment is \" + candi.lower() + \" for \" + term.lower() + \" .\" for candi in\n",
    "        #                candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list4.append(score)\n",
    "\n",
    "        # target_list = [\"The \" + term.lower() + \" is \" + candi.lower() + \" .\" for candi in\n",
    "        #                candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list5.append(score)\n",
    "\n",
    "        # score_list = [(score_list1[i] + score_list2[i] + score_list3[i]) for i in range(0, len(score_list1))]\n",
    "        print(score_list)\n",
    "        predict = candidate_list[np.argmax(score_list)]\n",
    "        predicted_sentence = target_list[np.argmax(score_list)]\n",
    "        true_labels.append(golden_polarity)\n",
    "        predicted_labels.append(predict)\n",
    "        #predicted_term = term.lower()\n",
    "        if predict == golden_polarity:\n",
    "            count += 1\n",
    "            print(line, predicted_sentence, \" actual:\", golden_polarity, \"acc:\", count/total, count, total)\n",
    "        else:\n",
    "            print(\"Mismatch:\",line,\" \", predicted_sentence, \" actual:\", golden_polarity,\"acc:\", count/total, count, total)\n",
    "\n",
    "    #matrix = self.compute_metrics(np.array(true_labels), np.array(predicted_labels), calculate_metrics=calculate_metrics)\n",
    "    #precision, recall, f1_score = matrix['calculate_metrics']\n",
    "    #precision, recall, f1_score = calculate_metrics(np.array(predicted_labels), np.array(true_labels))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=1.0)\n",
    "    loss = log_loss(true_labels, predicted_labels)\n",
    "    #classificaiton report \n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(f\"Loss: {loss}, Precision: {precision}, Recall: {recall}, F1-Score: {f1}, Acc: {accuracy}\")\n",
    "    print(\"confusion metrix:\", cm)\n",
    "    print(\"classification report:\",classification_report(true_labels,predicted_labels, zero_division=1.0))\n",
    "\n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict_test(model, device):#, output_dir):\n",
    "    candidate_list = [\"positive\", \"neutral\", \"negative\"]\n",
    "\n",
    "    #model = MBartForConditionalGeneration.from_pretrained(output_dir)\n",
    "    #model = MBartForConditionalGeneration.from_pretrained('/ICTeval/ICTeval_code/outputs/checkpoint-5865-epoch-15/mbart')\n",
    "    model.eval()\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    # tokenizer = MBart50TokenizerFast.from_pretrained('facebook/mbart-large-50')\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "    #tokenizer = GPT2Tokenizer.from_pretrained(\"sberbank-ai/mGPT\")\n",
    "\n",
    "    file_path = path\n",
    "    with open(file_path, \"r\") as f:\n",
    "        file = f.readlines()\n",
    "    train_data = []\n",
    "    count = 0\n",
    "    total = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for line in file:\n",
    "        total += 1\n",
    "        # score_list = []\n",
    "        score_list1 = []\n",
    "        score_list2 = []\n",
    "        score_list3 = []\n",
    "        score_list4 = []\n",
    "        score_list5 = []\n",
    "        line = line.strip()\n",
    "        x, term, golden_polarity = line.split(\"\\001\")[0], line.split(\"\\001\")[1], line.split(\"\\001\")[2]\n",
    "        input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids'].to(device)\n",
    "        #sentence = tokenizer.decode([input_ids], skip_special_tokens=True)\n",
    "        #print(\"sentence: \", sentence)\n",
    "        #print(\"input_ID:\", input_ids)\n",
    "        # target_list = [\"For \" + term.lower() + \", the sentiment is \" + candi.lower() + \" .\" for candi in candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list1.append(score)\n",
    "\n",
    "        target_list = [\"The sentiment polarity of \" + term.lower() + \" is \" + candi.lower() + \" .\" for candi in\n",
    "                       candidate_list]\n",
    "        #print(\"target_lsit\",target_list)\n",
    "        ## target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        ## input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "\n",
    "        output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids'].to(device)\n",
    "        model.to(device)\n",
    "        output_ids = output_ids.to(device)\n",
    "        print(\"logit and output_ID: \",output_ids)\n",
    "        #sentence2 = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "        #print(\"output ID:\", sentence2)\n",
    "        #print(\"Model device:\", model.device)\n",
    "        #print(\"Input tensor device:\", input_ids.device)\n",
    "        #print(\"Output tensor device:\", output_ids.device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, decoder_input_ids=output_ids)[0]\n",
    "            #print(np.shape(output))\n",
    "            logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "            print(\"LOGIT:::\",logits.shape)\n",
    "        #with torch.no_grad():\n",
    "        #    output = model(input_ids=input_ids.to(device))[0]\n",
    "        #    logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        for i in range(3):\n",
    "            score = 0\n",
    "            for j in range(logits[i].shape[0] - 2):\n",
    "                #print(logits[i][j][output_ids[i][j + 1]])\n",
    "                score += np.log(logits[i][j][output_ids[i][j + 1]])\n",
    "            score_list2.append(score)\n",
    "        score_list = score_list2\n",
    "\n",
    "        # target_list = [\"The \" + term.lower() + \" category has a \" + candi.lower() + \" label .\" for candi in\n",
    "        #                candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list3.append(score)\n",
    "\n",
    "        # target_list = [\"The sentiment is \" + candi.lower() + \" for \" + term.lower() + \" .\" for candi in\n",
    "        #                candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list4.append(score)\n",
    "\n",
    "        # target_list = [\"The \" + term.lower() + \" is \" + candi.lower() + \" .\" for candi in\n",
    "        #                candidate_list]\n",
    "        # # target_list = [\"For \" + term.lower() + \", it is a \" + candi.lower() + \"sentence .\" for candi in candidate_list]\n",
    "        # # input_ids = tokenizer([x] * 3, return_tensors='pt')['input_ids']\n",
    "        # output_ids = tokenizer(target_list, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        # with torch.no_grad():\n",
    "        #     output = model(input_ids=input_ids.to(device), decoder_input_ids=output_ids.to(device))[0]\n",
    "        #     logits = output.softmax(dim=-1).to('cpu').numpy()\n",
    "        # for i in range(3):\n",
    "        #     score = 1\n",
    "        #     for j in range(logits[i].shape[0] - 2):\n",
    "        #         score *= logits[i][j][output_ids[i][j + 1]]\n",
    "        #     score_list5.append(score)\n",
    "\n",
    "        # score_list = [(score_list1[i] + score_list2[i]) for i in range(0, len(score_list1))]\n",
    "        predict = candidate_list[np.argmax(score_list)]\n",
    "        predicted_sentence = target_list[np.argmax(score_list)]\n",
    "        true_labels.append(golden_polarity)\n",
    "        predicted_labels.append(predict)\n",
    "        # with open(\"ICTeval_predict_output/cnn_test2_og_backtrans_AI_predict.txt\", 'a') as f0:\n",
    "        #            f0.writelines(line + \"\\n\")\n",
    "        #            f0.writelines(predicted_sentence + \"\\n\")\n",
    "        # with open(\"ICTeval_predict_output/cnn_test2_og_backtrans_AI_true.txt\", 'a') as f0:\n",
    "        #            f0.writelines(line.split(\"\\001\")[0] + \"\\n\")\n",
    "        #            f0.writelines(\"The sentiment polarity of \" + term.lower() + \" is \" + golden_polarity + \" .\"+ \"\\n\")\n",
    "        if predict == golden_polarity:\n",
    "            count += 1\n",
    "            print(line, term, \" \", predicted_sentence,\"actual:\", predict, golden_polarity,\"acc:\", count/total, count, total)\n",
    "        else:\n",
    "            print(\"Mismatch:\",line, \" \", term , \" \", predicted_sentence, \" actual:\", predict, golden_polarity,\"acc: \" ,count/total, count, total)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=1.0)    \n",
    "    #precision, recall, f1_score = calculate_metrics(np.array(predicted_labels), np.array(true_labels))\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(f\"Precision: {precision}, Recall: {recall}, F1-Score: {f1}, Acc: {accuracy}\")\n",
    "    print(\"confusion metrix:\", cm)\n",
    "    print(\"classification report:\",classification_report(true_labels,predicted_labels, zero_division=1.0))\n",
    "    #print(predicted_labels)\n",
    "    #print(true_labels)\n",
    "\n",
    "    result = {\"File\": file_path, \"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1\": f1}\n",
    "    # return file_path, accuracy, precision, recall, f1\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json             model.safetensors        tokenizer_config.json\n",
      "generation_config.json  model_args.json          training_args.bin\n",
      "merges.txt              special_tokens_map.json  vocab.json\n"
     ]
    }
   ],
   "source": [
    "ls outputs/Original/1-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/outputs/Original/1-fold'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:402\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/outputs/Original/1-fold'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mBartForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# model2 = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#MBartForConditionalGeneration\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# model2 = GPT2LMHeadModel.from_pretrained(\"sberbank-ai/mGPT\").to(device)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# model2 = GPT2Model.from_pretrained('outputs/mbart').to(device)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m acc \u001b[38;5;241m=\u001b[39m predict_test(model2, device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:3247\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   3246\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 3247\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3254\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3255\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3257\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3258\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3260\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3261\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3262\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   3263\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:466\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/outputs/Original/1-fold'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model2 = BartForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "# model2 = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn').to(device)\n",
    "\n",
    "#MBartForConditionalGeneration\n",
    "# model2 = GPT2LMHeadModel.from_pretrained(\"sberbank-ai/mGPT\").to(device)\n",
    "# model2 = GPT2Model.from_pretrained('outputs/mbart').to(device)\n",
    "acc = predict_test(model2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'File': 'ICTeval_code/crossvalid/testformat/test4.txt',\n",
       " 'Accuracy': 0.7788778877887789,\n",
       " 'Precision': 0.9262926292629263,\n",
       " 'Recall': 0.3333333333333333,\n",
       " 'F1': 0.29189857761286336}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sinsin/ICTeval_code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sinsin/ICTeval_code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python+NGC on Kubernetes Operator 240916080250",
   "language": "python",
   "name": "jupyter-eg-kernel-k8s-cmjkop-ngc-py-1i7s5upl0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
